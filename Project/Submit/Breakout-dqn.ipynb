{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import pandas as pd\n",
    "import gym\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use Cuda</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Replay Buffer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Epsilon greedy exploration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialSchedule:\n",
    "    def __init__(self, value_from, value_to, num_steps):\n",
    "        \"\"\"Exponential schedule from `value_from` to `value_to` in `num_steps` steps.\n",
    "\n",
    "        $value(t) = a \\exp (b t)$\n",
    "\n",
    "        :param value_from: initial value\n",
    "        :param value_to: final value\n",
    "        :param num_steps: number of steps for the exponential schedule\n",
    "        \"\"\"\n",
    "        self.value_from = value_from\n",
    "        self.value_to = value_to\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # YOUR CODE HERE:  determine the `a` and `b` parameters such that the schedule is correct\n",
    "        self.a = self.value_from\n",
    "        self.b = np.log(self.a/self.value_to)/ (self.num_steps-1)\n",
    "\n",
    "    def value(self, step) -> float:\n",
    "        \"\"\"Return exponentially interpolated value between `value_from` and `value_to`interpolated value between.\n",
    "\n",
    "        returns {\n",
    "            `value_from`, if step == 0 or less\n",
    "            `value_to`, if step == num_steps - 1 or more\n",
    "            the exponential interpolation between `value_from` and `value_to`, if 0 <= steps < num_steps\n",
    "        }\n",
    "        \n",
    "        :param step:  The step at which to compute the interpolation.\n",
    "        :rtype: float.  The interpolated value.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE:  implement the schedule rule as described in the docstring,\n",
    "        # using attributes `self.a` and `self.b`.\n",
    "        #value = ...\n",
    "        \n",
    "        if step <= 0:\n",
    "            value = self.value_from\n",
    "            return value\n",
    "        \n",
    "        if step >= self.num_steps - 1:\n",
    "            value = self.value_to\n",
    "            return value\n",
    "        \n",
    "        value = self.a/np.exp(self.b*step)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Computing Temporal Difference Loss</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    #next_state = Variable(torch.FloatTensor(np.float32(next_state)), volatile=True)\n",
    "    with torch.no_grad():\n",
    "        next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    q_values      = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Atari Environment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrappers import make_atari, wrap_deepmind, wrap_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"BreakoutNoFrameskip-v4\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            #state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "            with torch.no_grad():\n",
    "                state = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0))\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "replay_initial = 10_000\n",
    "replay_buffer = ReplayBuffer(100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 2_000_000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "losses = []\n",
    "returns = []\n",
    "lengths = []\n",
    "episode_reward = []\n",
    "t_episode = 0\n",
    "i_episode = 0\n",
    "exploration = ExponentialSchedule(1.0, 0.05, 500_000)\n",
    "state = env.reset()\n",
    "pbar = tqdm.tnrange(num_frames, ncols='100%')\n",
    "# for frame_idx in range(1, num_frames + 1):\n",
    "for frame_idx in pbar:\n",
    "#     epsilon = epsilon_by_frame(frame_idx)\n",
    "    epsilon = exploration.value(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    episode_reward.append(reward)\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        G = 0\n",
    "        for i in reversed(episode_reward):\n",
    "            G = i + G * gamma\n",
    "        returns.append(G)\n",
    "        episode_reward = []\n",
    "        pbar.set_description(\n",
    "                f'Episode: {i_episode} | Steps: {t_episode + 1} | Return: {G:5.2f} | Epsilon: {epsilon:4.2f}'\n",
    "            )\n",
    "        lengths.append(t_episode+1)\n",
    "        t_episode = 0\n",
    "        i_episode += 1\n",
    "    else:\n",
    "        t_episode += 1\n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.item())\n",
    "#     if frame_idx % 10000 == 0:\n",
    "#         plot(frame_idx, all_rewards, losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array(lengths)\n",
    "b1 = np.array(returns)\n",
    "\n",
    "df = pd.DataFrame({\"lengths\" : a1, \"returns\" : b1})\n",
    "df.to_csv(\"breakout_dqn.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
